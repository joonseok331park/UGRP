# core/dataset.py (ì˜ì¡´ì„± ì œê±° ë° ì´ì¤‘ í´ë˜ìŠ¤ êµ¬ì¡° ì ìš© ìµœì¢… ë²„ì „)

import re
import torch
import random
import pandas as pd
from typing import List, Dict, Tuple
from itertools import chain
from torch.utils.data import Dataset

# ì´ íŒŒì¼ì€ ê°™ì€ core í´ë” ë‚´ì˜ tokenizerë§Œ ì˜ì¡´í•©ë‹ˆë‹¤.
from core.tokenizer import CANTokenizer

def _load_and_parse_log(file_path: str) -> pd.DataFrame:
    """
    CAN-MIRGU í˜•ì‹ì˜ ë¡œê·¸ íŒŒì¼ì„ ì½ê³  íŒŒì‹±í•˜ì—¬ DataFrameìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ë‚´ë¶€ í—¬í¼ í•¨ìˆ˜.
    (ê¸°ì¡´ utils.data_loader.load_can_dataì˜ ì—­í• ì„ ëŒ€ì²´í•©ë‹ˆë‹¤)

    :param file_path: .log íŒŒì¼ì˜ ê²½ë¡œ.
    :return: 'CAN_ID', 'Data', 'Label' ì»¬ëŸ¼ì„ ê°€ì§„ pandas DataFrame.
    """
    # ì •ê·œ í‘œí˜„ì‹ì„ ì‚¬ìš©í•˜ì—¬ ë¡œê·¸ ë¼ì¸ì„ íš¨ìœ¨ì ìœ¼ë¡œ íŒŒì‹±í•©ë‹ˆë‹¤.
    # í¬ë§·: (timestamp) can0 CAN_ID#DATA LABEL
    log_pattern = re.compile(r'\((?P<timestamp>\d+\.\d+)\)\s+can0\s+(?P<can_id>[0-9A-Fa-f]{3})#(?P<data>[0-9A-Fa-f]{0,16})\s+(?P<label>[01])')
    
    parsed_data = []
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            match = log_pattern.match(line)
            if match:
                d = match.groupdict()
                
                # ë°ì´í„° í˜ì´ë¡œë“œë¥¼ 8ë°”ì´íŠ¸(16ì§„ìˆ˜ 16ê¸€ì)ë¡œ íŒ¨ë”©í•©ë‹ˆë‹¤.
                # Jo & Kim (2024)ì˜ <VOID> í† í° ê°œë…ê³¼ LSF-IDM (2023)ì˜ '00' íŒ¨ë”© ê°œë…ì„
                # ì‹¤ì œ ë°ì´í„° ì²˜ë¦¬ ë¡œì§ìœ¼ë¡œ êµ¬í˜„í•œ ê²ƒì…ë‹ˆë‹¤.
                padded_data = d['data'].ljust(16, '0')
                
                # 2ê¸€ìì”© ì˜ë¼ 8ë°”ì´íŠ¸ ë¦¬ìŠ¤íŠ¸ë¡œ ë§Œë“­ë‹ˆë‹¤.
                data_bytes = [padded_data[i:i+2] for i in range(0, 16, 2)]
                
                parsed_data.append({
                    'CAN_ID': d['can_id'],
                    'Data': data_bytes,
                    'Label': int(d['label'])
                })
    return pd.DataFrame(parsed_data)


# class MLMDataset(Dataset):
#     """
#     BERT ëª¨ë¸ì˜ MLM ì‚¬ì „ í›ˆë ¨ì„ ìœ„í•œ ë°ì´í„°ì…‹.
#     CAN-MIRGU ë³´ê³ ì„œì— ëª…ì‹œëœ 'ì •ìƒ(Benign)' ë°ì´í„° íŒŒì¼ ì²˜ë¦¬ì— ìµœì í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
#     """
#     def __init__(self, file_path: str, tokenizer: CANTokenizer, seq_len: int, mask_prob: float = 0.15):
#         self.tokenizer = tokenizer
#         self.seq_len = seq_len
#         self.mask_prob = mask_prob
        
#         print(f"MLMDataset: Loading and tokenizing data from file: {file_path}...")
#         can_df = _load_and_parse_log(file_path)
        
#         if can_df.empty:
#             print(f"Warning: No data loaded from {file_path}. MLMDataset will be empty.")
#             self.token_id_stream = []
#         else:
#             # DataFrameì„ í† í° ë¦¬ìŠ¤íŠ¸ì˜ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ (Label ì»¬ëŸ¼ì€ ì˜ë„ì ìœ¼ë¡œ ë¬´ì‹œ)
#             all_frames_as_tokens = []
#             for _, row in can_df.iterrows():
#                 can_id_token = str(int(row['CAN_ID'], 16) + self.tokenizer.ID_OFFSET)
#                 frame_tokens = [can_id_token] + row['Data']
#                 all_frames_as_tokens.append(frame_tokens)
            
#             token_stream = list(chain.from_iterable(all_frames_as_tokens))
#             self.token_id_stream = self.tokenizer.encode(token_stream)

#         # íŠ¹ìˆ˜ í† í° ID ë¯¸ë¦¬ ì €ì¥
#         self.mask_token_id = tokenizer.token_to_id['<MASK>']
#         self.pad_token_id = tokenizer.token_to_id['<PAD>']
#         self.vocab_size = tokenizer.get_vocab_size()
#         self.special_token_ids = {tokenizer.token_to_id[token] for token in tokenizer.special_tokens}
        
#         print(f"MLMDataset: Init complete. Token stream length: {len(self.token_id_stream):,}")

#     def __len__(self) -> int:
#         if not self.token_id_stream or len(self.token_id_stream) < self.seq_len:
#             return 0
#         return len(self.token_id_stream) - self.seq_len + 1

#     def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:
#         sequence = self.token_id_stream[idx : idx + self.seq_len]
#         input_ids = list(sequence)
#         labels = [-100] * self.seq_len

#         candidate_indices = [i for i, token_id in enumerate(input_ids) if token_id not in self.special_token_ids]
#         num_to_mask = int(len(candidate_indices) * self.mask_prob)
#         if num_to_mask > 0:
#             masked_indices = random.sample(candidate_indices, num_to_mask)
#             for i in masked_indices:
#                 labels[i] = input_ids[i]
#                 rand = random.random()
#                 if rand < 0.8:
#                     input_ids[i] = self.mask_token_id
#                 elif rand < 0.9:
#                     random_token_id = random.randint(len(self.special_token_ids), self.vocab_size - 1)
#                     input_ids[i] = random_token_id
        
#         attention_mask = [1] * self.seq_len

#         return {
#             'input_ids': torch.tensor(input_ids, dtype=torch.long),
#             'attention_mask': torch.tensor(attention_mask, dtype=torch.long),
#             'labels': torch.tensor(labels, dtype=torch.long)
#         }

class ClassificationDataset(Dataset):
    """
    ë¯¸ì„¸ ì¡°ì •(Fine-tuning) ë° ì§€ì‹ ì¦ë¥˜(Knowledge Distillation)ë¥¼ ìœ„í•œ ë¶„ë¥˜ ë°ì´í„°ì…‹.
    CAN-MIRGU ë³´ê³ ì„œì— ëª…ì‹œëœ 'ê³µê²©(Attack)' ë°ì´í„° ì²˜ë¦¬ì— ì í•©í•©ë‹ˆë‹¤.
    """
    def __init__(self, file_path: str, tokenizer: CANTokenizer, seq_len: int, stride: int = 1):
        self.tokenizer = tokenizer
        self.seq_len = seq_len
        self.sequences = []
        self.labels = []
        
        print(f"ClassificationDataset: Loading and preparing sequences from: {file_path}...")
        can_df = _load_and_parse_log(file_path)
        
        if not can_df.empty:
            all_frames_as_tokens = []
            frame_labels = []
            for _, row in can_df.iterrows():
                can_id_token = str(int(row['CAN_ID'], 16) + self.tokenizer.ID_OFFSET)
                frame_tokens = [can_id_token] + row['Data']
                all_frames_as_tokens.append(frame_tokens)
                frame_labels.append(row['Label'])

            # ê° í”„ë ˆì„ì€ 9ê°œì˜ í† í°ìœ¼ë¡œ êµ¬ì„±ë¨
            num_tokens_per_frame = 9
            token_stream = list(chain.from_iterable(all_frames_as_tokens))
            encoded_stream = self.tokenizer.encode(token_stream)
            
            # ìŠ¬ë¼ì´ë”© ìœˆë„ìš°ë¡œ ì‹œí€€ìŠ¤ì™€ ë ˆì´ë¸” ìƒì„±
            for i in range(len(encoded_stream) - seq_len + 1, stride):
                self.sequences.append(encoded_stream[i : i + seq_len])
                
                # ì‹œí€€ìŠ¤ì— í•´ë‹¹í•˜ëŠ” ì›ë³¸ í”„ë ˆì„ì˜ ë ˆì´ë¸” ë²”ìœ„ë¥¼ ê³„ì‚°
                start_frame_idx = i // num_tokens_per_frame
                end_frame_idx = (i + seq_len -1) // num_tokens_per_frame + 1
                
                # ì‹œí€€ìŠ¤ ë‚´ì— ê³µê²© í”„ë ˆì„ì´ í•˜ë‚˜ë¼ë„ ìˆìœ¼ë©´ ì‹œí€€ìŠ¤ì˜ ë ˆì´ë¸”ì€ 1(ê³µê²©)
                sequence_label = 1 if any(frame_labels[start_frame_idx:end_frame_idx]) else 0
                self.labels.append(sequence_label)
        
        print(f"ClassificationDataset: Init complete. Number of sequences: {len(self.sequences):,}")

    def __len__(self) -> int:
        return len(self.sequences)

    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:
        return {
            'input_ids': torch.tensor(self.sequences[idx], dtype=torch.long),
            'attention_mask': torch.tensor([1] * self.seq_len, dtype=torch.long),
            'labels': torch.tensor(self.labels[idx], dtype=torch.long)
        }

if __name__ == '__main__':
    # --- ì‹¤ì œ ë°ì´í„° ê¸°ë°˜ í…ŒìŠ¤íŠ¸ í™˜ê²½ ì„¤ì • ---
    import os
    import numpy as np
    from pathlib import Path

    # 1. ì‹¤ì œ ë°ì´í„° íŒŒì¼ ê²½ë¡œ ì§€ì •
    #    [ì£¼ì˜] ì´ íŒŒì¼ì´ ì‹¤ì œë¡œ ì¡´ì¬í•´ì•¼ í•©ë‹ˆë‹¤. ê²½ë¡œê°€ ë‹¤ë¥´ë‹¤ë©´ ìˆ˜ì •í•´ ì£¼ì‹­ì‹œì˜¤.
    #    ìš°ì„ , ë³´ê³ ì„œì— ëª…ì‹œëœ Benign ë°ì´í„° ì¤‘ í•˜ë‚˜ë¥¼ ëŒ€ìƒìœ¼ë¡œ í•©ë‹ˆë‹¤.
    try:
        # í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ìƒëŒ€ ê²½ë¡œ ì„¤ì •
        project_root = Path(__file__).parent.parent 
        # ì‹¤ì œ íŒŒì¼ëª…ì„ ì •í™•íˆ ì•Œ ìˆ˜ ì—†ìœ¼ë¯€ë¡œ, ì¼ë°˜ì ì¸ ì´ë¦„ìœ¼ë¡œ ê°€ì •í•©ë‹ˆë‹¤.
        # ì—°êµ¬ì›ë‹˜ì˜ ì‹¤ì œ íŒŒì¼ëª…ìœ¼ë¡œ ì´ ë¶€ë¶„ì„ ìˆ˜ì •í•´ì•¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        real_data_path = project_root / "data" / "raw" / "can_mirgu" / "Benign" / "Day_1" / "Benign_day1_file1.log"
        NUM_LINES_TO_TEST = 100000

        if not real_data_path.exists():
            raise FileNotFoundError

    except FileNotFoundError:
        print("="*60)
        print(f"âš ï¸  í…ŒìŠ¤íŠ¸ ê²½ê³ : ì‹¤ì œ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print(f"   - í™•ì¸í•œ ê²½ë¡œ: {real_data_path.resolve()}")
        print(f"   - `dataset.py`ì˜ í…ŒìŠ¤íŠ¸ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ë ¤ë©´ ìœ„ ê²½ë¡œì— ì‹¤ì œ ë¡œê·¸ íŒŒì¼ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        print("="*60)
        # ì‹¤ì œ íŒŒì¼ì´ ì—†ìœ¼ë©´ í…ŒìŠ¤íŠ¸ë¥¼ ì§„í–‰í•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ ì¢…ë£Œí•©ë‹ˆë‹¤.
        exit()


    # 2. ì‹¤ì œ ë°ì´í„° ì¼ë¶€ë¥¼ ì½ì–´ ê°€ìƒì˜ í…ŒìŠ¤íŠ¸ ë¡œê·¸ íŒŒì¼ ìƒì„±
    temp_dir = Path("./temp_test")
    temp_dir.mkdir(exist_ok=True)
    test_file_path = temp_dir / "test_real_data_snippet.log"

    with open(real_data_path, 'r', encoding='utf-8') as f_real:
        lines = [f_real.readline() for _ in range(NUM_LINES_TO_TEST)]

    with open(test_file_path, "w", encoding="utf-8") as f_temp:
        f_temp.writelines(lines)

    print(f"--- `_load_and_parse_log` ì‹¤ì œ ë°ì´í„° ê¸°ë°˜ í…ŒìŠ¤íŠ¸ ---")
    print(f"ëŒ€ìƒ íŒŒì¼: {real_data_path.name}")
    print(f"ì•ë¶€ë¶„ {NUM_LINES_TO_TEST}ì¤„ì„ ì„ì‹œ íŒŒì¼ë¡œ ë³µì‚¬í•˜ì—¬ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.")

    # 3. íŒŒì„œ í•¨ìˆ˜ ì‹¤í–‰
    try:
        df = _load_and_parse_log(test_file_path)
        
        # 4. ê²°ê³¼ ê²€ì¦
        print("\nê²€ì¦ ì‹œì‘...")

        # 4.1. ë°ì´í„°ê°€ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆëŠ”ì§€ í™•ì¸
        assert not df.empty, "ì˜¤ë¥˜: íŒŒì‹± í›„ DataFrameì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. íŒŒì¼ ë‚´ìš©ì´ë‚˜ íŒŒì‹± ë¡œì§ì„ í™•ì¸í•˜ì„¸ìš”."
        print(f"âœ… [ì„±ê³µ] ë°ì´í„° ë¡œë“œ ì™„ë£Œ ({len(df)}/{NUM_LINES_TO_TEST} ë¼ì¸ íŒŒì‹±)")
        
        print("\níŒŒì‹± ê²°ê³¼ ì¼ë¶€ (ìƒìœ„ 5ê°œ í–‰):")
        print(df.head())

        # 4.2. DataFrame êµ¬ì¡° ë° íƒ€ì… ê²€ì¦
        assert all(col in df.columns for col in ['CAN_ID', 'Data', 'Label']), "ì˜¤ë¥˜: í•„ìˆ˜ ì»¬ëŸ¼ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤."
        print("âœ… [ì„±ê³µ] í•„ìˆ˜ ì»¬ëŸ¼(CAN_ID, Data, Label) ì¡´ì¬ ì—¬ë¶€ ê²€ì¦ ì™„ë£Œ.")
        
        first_row = df.iloc[0]
        assert isinstance(first_row['CAN_ID'], str) and len(first_row['CAN_ID']) == 3, "ì˜¤ë¥˜: CAN_ID í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤ (3ìë¦¬ ë¬¸ìì—´ì´ì–´ì•¼ í•¨)."
        assert isinstance(first_row['Data'], list) and len(first_row['Data']) == 8, "ì˜¤ë¥˜: Data í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤ (8ê°œ ìš”ì†Œë¥¼ ê°€ì§„ ë¦¬ìŠ¤íŠ¸ì—¬ì•¼ í•¨)."
        assert isinstance(first_row['Label'], np.integer), "ì˜¤ë¥˜: Label í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤ (ì •ìˆ˜ì—¬ì•¼ í•¨)."
        print("âœ… [ì„±ê³µ] ë°ì´í„° íƒ€ì… ë° í˜•ì‹ ê²€ì¦ ì™„ë£Œ.")

        print("\nğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼! `_load_and_parse_log` í•¨ìˆ˜ê°€ ì‹¤ì œ ë°ì´í„°ì— ëŒ€í•´ ì •ìƒì ìœ¼ë¡œ ë™ì‘í•©ë‹ˆë‹¤.")

    finally:
        # 5. í…ŒìŠ¤íŠ¸ ì¢…ë£Œ í›„ ê°€ìƒ íŒŒì¼ ë° ë””ë ‰í† ë¦¬ ì‚­ì œ
        if os.path.exists(test_file_path):
            os.remove(test_file_path)
        if os.path.exists(temp_dir):
            os.rmdir(temp_dir)
        print(f"\ní…ŒìŠ¤íŠ¸ ì™„ë£Œ í›„ ì„ì‹œ íŒŒì¼ ë° ë””ë ‰í† ë¦¬ ì‚­ì œë¨.")