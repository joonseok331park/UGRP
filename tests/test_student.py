# UGRP/tests/test_student.py
"""
BiLSTM í•™ìƒ ëª¨ë¸ì˜ í¬ê´„ì ì¸ í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸.
ì—£ì§€ ì¼€ì´ìŠ¤, ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬, ë©”ëª¨ë¦¬ í”„ë¡œíŒŒì¼ë§ì„ í¬í•¨í•©ë‹ˆë‹¤.
"""

import torch
import torch.nn as nn
import time
import tracemalloc
import sys
import os
from typing import Dict, Tuple, Any

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from models.student import BiLSTMStudent, get_student_hyperparams


class TestStudentModel:
    """BiLSTM í•™ìƒ ëª¨ë¸ì„ ìœ„í•œ ì¢…í•© í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.vocab_size = 2309
        self.num_classes = 2
        self.pad_token_id = 0
        self.seq_len = 126
        self.batch_size = 32
        
        # í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì €ì¥
        self.results: Dict[str, Any] = {}
        
    def setup_model(self) -> BiLSTMStudent:
        """í…ŒìŠ¤íŠ¸ìš© ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
        return BiLSTMStudent(
            vocab_size=self.vocab_size,
            num_classes=self.num_classes,
            pad_token_id=self.pad_token_id
        )
    
    def test_model_creation(self) -> bool:
        """í…ŒìŠ¤íŠ¸ 1: ëª¨ë¸ ìƒì„± ë° ê¸°ë³¸ ì†ì„± ê²€ì¦"""
        print("\n[TEST 1] ëª¨ë¸ ìƒì„± í…ŒìŠ¤íŠ¸")
        print("-" * 50)
        
        try:
            model = self.setup_model()
            
            # í•˜ì´í¼íŒŒë¼ë¯¸í„° ê²€ì¦
            hyperparams = get_student_hyperparams()
            assert hyperparams["embedding_dim"] == 64, "ì„ë² ë”© ì°¨ì› ë¶ˆì¼ì¹˜"
            assert hyperparams["hidden_size"] == 64, "Hidden size ë¶ˆì¼ì¹˜"
            assert hyperparams["num_layers"] == 2, "ë ˆì´ì–´ ìˆ˜ ë¶ˆì¼ì¹˜"
            assert hyperparams["dropout"] == 0.2, "ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨ ë¶ˆì¼ì¹˜"
            
            print("âœ… ëª¨ë¸ ìƒì„± ì„±ê³µ")
            print(f"   - ì„ë² ë”© ì°¨ì›: {hyperparams['embedding_dim']}")
            print(f"   - Hidden size: {hyperparams['hidden_size']}")
            print(f"   - ë ˆì´ì–´ ìˆ˜: {hyperparams['num_layers']}")
            print(f"   - ë“œë¡­ì•„ì›ƒ: {hyperparams['dropout']}")
            
            self.results["model_creation"] = "PASS"
            return True
            
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {e}")
            self.results["model_creation"] = "FAIL"
            return False
    
    def test_parameter_count(self) -> bool:
        """í…ŒìŠ¤íŠ¸ 2: íŒŒë¼ë¯¸í„° ìˆ˜ ë° ëª¨ë¸ í¬ê¸° ê²€ì¦"""
        print("\n[TEST 2] íŒŒë¼ë¯¸í„° ìˆ˜ ë° ëª¨ë¸ í¬ê¸°")
        print("-" * 50)
        
        try:
            model = self.setup_model()
            
            # íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°
            total_params = sum(p.numel() for p in model.parameters())
            trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
            
            # ëª¨ë¸ í¬ê¸° ê³„ì‚° (MB)
            param_size = sum(p.numel() * p.element_size() for p in model.parameters())
            buffer_size = sum(b.numel() * b.element_size() for b in model.buffers())
            model_size_mb = (param_size + buffer_size) / 1024 / 1024
            
            print(f"âœ… ì´ íŒŒë¼ë¯¸í„° ìˆ˜: {total_params:,}")
            print(f"âœ… í•™ìŠµ ê°€ëŠ¥ íŒŒë¼ë¯¸í„°: {trainable_params:,}")
            print(f"âœ… ëª¨ë¸ í¬ê¸°: {model_size_mb:.2f} MB")
            
            # ê²½ëŸ‰í™” ëª©í‘œ ê²€ì¦ (< 300,000 íŒŒë¼ë¯¸í„°)
            assert trainable_params < 300_000, f"íŒŒë¼ë¯¸í„° ìˆ˜ ì´ˆê³¼: {trainable_params}"
            # ëª¨ë¸ í¬ê¸° ëª©í‘œ ê²€ì¦ (< 16MB)
            assert model_size_mb < 16, f"ëª¨ë¸ í¬ê¸° ì´ˆê³¼: {model_size_mb} MB"
            
            self.results["parameter_count"] = trainable_params
            self.results["model_size_mb"] = model_size_mb
            return True
            
        except Exception as e:
            print(f"âŒ íŒŒë¼ë¯¸í„° ê²€ì¦ ì‹¤íŒ¨: {e}")
            self.results["parameter_count"] = "FAIL"
            return False
    
    def test_normal_forward(self) -> bool:
        """í…ŒìŠ¤íŠ¸ 3: ì •ìƒì ì¸ ìˆœì „íŒŒ í…ŒìŠ¤íŠ¸"""
        print("\n[TEST 3] ì •ìƒ ìˆœì „íŒŒ í…ŒìŠ¤íŠ¸")
        print("-" * 50)
        
        try:
            model = self.setup_model()
            model.eval()
            
            # ì •ìƒ ì…ë ¥ ìƒì„±
            input_ids = torch.randint(1, self.vocab_size, (self.batch_size, self.seq_len))
            attention_mask = torch.ones(self.batch_size, self.seq_len, dtype=torch.long)
            
            with torch.no_grad():
                output = model(input_ids, attention_mask)
            
            expected_shape = (self.batch_size, self.num_classes)
            assert output.shape == expected_shape, f"ì¶œë ¥ shape ë¶ˆì¼ì¹˜: {output.shape} != {expected_shape}"
            
            print(f"âœ… ì…ë ¥ shape: {input_ids.shape}")
            print(f"âœ… ì¶œë ¥ shape: {output.shape}")
            print(f"âœ… ì •ìƒ ìˆœì „íŒŒ ì„±ê³µ")
            
            self.results["normal_forward"] = "PASS"
            return True
            
        except Exception as e:
            print(f"âŒ ìˆœì „íŒŒ ì‹¤íŒ¨: {e}")
            self.results["normal_forward"] = "FAIL"
            return False
    
    def test_variable_length_sequences(self) -> bool:
        """í…ŒìŠ¤íŠ¸ 4: ê°€ë³€ ê¸¸ì´ ì‹œí€€ìŠ¤ ì²˜ë¦¬"""
        print("\n[TEST 4] ê°€ë³€ ê¸¸ì´ ì‹œí€€ìŠ¤ í…ŒìŠ¤íŠ¸")
        print("-" * 50)
        
        try:
            model = self.setup_model()
            model.eval()
            
            # ë‹¤ì–‘í•œ ê¸¸ì´ì˜ ì‹œí€€ìŠ¤ ìƒì„±
            batch_size = 4
            input_ids = torch.tensor([
                [10, 20, 30, 40, 50, 0, 0, 0],  # ê¸¸ì´ 5
                [15, 25, 35, 0, 0, 0, 0, 0],     # ê¸¸ì´ 3
                [18, 28, 38, 48, 58, 68, 78, 88], # ê¸¸ì´ 8 (ìµœëŒ€)
                [11, 0, 0, 0, 0, 0, 0, 0]        # ê¸¸ì´ 1
            ])
            
            attention_mask = torch.tensor([
                [1, 1, 1, 1, 1, 0, 0, 0],
                [1, 1, 1, 0, 0, 0, 0, 0],
                [1, 1, 1, 1, 1, 1, 1, 1],
                [1, 0, 0, 0, 0, 0, 0, 0]
            ], dtype=torch.long)
            
            with torch.no_grad():
                output = model(input_ids, attention_mask)
            
            assert output.shape == (batch_size, self.num_classes)
            
            seq_lengths = attention_mask.sum(dim=1).tolist()
            print(f"âœ… ì‹œí€€ìŠ¤ ê¸¸ì´: {seq_lengths}")
            print(f"âœ… ì¶œë ¥ shape: {output.shape}")
            print(f"âœ… ê°€ë³€ ê¸¸ì´ ì²˜ë¦¬ ì„±ê³µ")
            
            self.results["variable_length"] = "PASS"
            return True
            
        except Exception as e:
            print(f"âŒ ê°€ë³€ ê¸¸ì´ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            self.results["variable_length"] = "FAIL"
            return False
    
    def test_edge_cases(self) -> bool:
        """í…ŒìŠ¤íŠ¸ 5: ì—£ì§€ ì¼€ì´ìŠ¤ (ê·¹ë‹¨ì  ìƒí™©)"""
        print("\n[TEST 5] ì—£ì§€ ì¼€ì´ìŠ¤ í…ŒìŠ¤íŠ¸")
        print("-" * 50)
        
        try:
            model = self.setup_model()
            model.eval()
            
            # ì¼€ì´ìŠ¤ 1: ëª¨ë“  í† í°ì´ íŒ¨ë”©
            print("  [5-1] ëª¨ë“  í† í°ì´ íŒ¨ë”©ì¸ ê²½ìš°")
            all_padding = torch.zeros((1, 10), dtype=torch.long)
            all_padding_mask = torch.zeros((1, 10), dtype=torch.long)
            
            with torch.no_grad():
                output1 = model(all_padding, all_padding_mask)
            assert output1.shape == (1, self.num_classes)
            print("  âœ… ëª¨ë“  íŒ¨ë”© ì²˜ë¦¬ ì„±ê³µ")
            
            # ì¼€ì´ìŠ¤ 2: ìµœëŒ€ ê¸¸ì´ ì‹œí€€ìŠ¤
            print("  [5-2] ìµœëŒ€ ê¸¸ì´ ì‹œí€€ìŠ¤")
            max_length = torch.randint(1, self.vocab_size, (1, self.seq_len))
            max_mask = torch.ones(1, self.seq_len, dtype=torch.long)
            
            with torch.no_grad():
                output2 = model(max_length, max_mask)
            assert output2.shape == (1, self.num_classes)
            print("  âœ… ìµœëŒ€ ê¸¸ì´ ì²˜ë¦¬ ì„±ê³µ")
            
            # ì¼€ì´ìŠ¤ 3: ë‹¨ì¼ í† í°ë§Œ ìœ íš¨
            print("  [5-3] ë‹¨ì¼ ìœ íš¨ í† í°")
            single_valid = torch.zeros((1, 10), dtype=torch.long)
            single_valid[0, 0] = 100
            single_mask = torch.zeros((1, 10), dtype=torch.long)
            single_mask[0, 0] = 1
            
            with torch.no_grad():
                output3 = model(single_valid, single_mask)
            assert output3.shape == (1, self.num_classes)
            print("  âœ… ë‹¨ì¼ í† í° ì²˜ë¦¬ ì„±ê³µ")
            
            # ì¼€ì´ìŠ¤ 4: attention_mask ì—†ì´ ì‹¤í–‰
            print("  [5-4] attention_mask ì—†ëŠ” ê²½ìš°")
            no_mask_input = torch.randint(1, self.vocab_size, (2, 8))
            
            with torch.no_grad():
                output4 = model(no_mask_input, attention_mask=None)
            assert output4.shape == (2, self.num_classes)
            print("  âœ… ë§ˆìŠ¤í¬ ì—†ì´ ì²˜ë¦¬ ì„±ê³µ")
            
            self.results["edge_cases"] = "PASS"
            return True
            
        except Exception as e:
            print(f"âŒ ì—£ì§€ ì¼€ì´ìŠ¤ ì‹¤íŒ¨: {e}")
            print(f"   ì˜¤ë¥˜ ìƒì„¸: {traceback.format_exc()}")
            self.results["edge_cases"] = "FAIL"
            return False
    
    def test_inference_speed(self) -> bool:
        """í…ŒìŠ¤íŠ¸ 6: ì¶”ë¡  ì†ë„ ë²¤ì¹˜ë§ˆí¬"""
        print("\n[TEST 6] ì¶”ë¡  ì†ë„ ë²¤ì¹˜ë§ˆí¬")
        print("-" * 50)
        
        try:
            model = self.setup_model()
            model.eval()
            
            # GPU ì‚¬ìš© ê°€ëŠ¥ ì‹œ GPUë¡œ ì´ë™
            device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
            model = model.to(device)
            
            # í…ŒìŠ¤íŠ¸ ì…ë ¥ ì¤€ë¹„
            input_ids = torch.randint(1, self.vocab_size, (1, self.seq_len)).to(device)
            attention_mask = torch.ones(1, self.seq_len, dtype=torch.long).to(device)
            
            # ì›Œë°ì—… (GPU ì´ˆê¸°í™”)
            for _ in range(10):
                with torch.no_grad():
                    _ = model(input_ids, attention_mask)
            
            # ì¶”ë¡  ì‹œê°„ ì¸¡ì • (100íšŒ í‰ê· )
            num_iterations = 100
            torch.cuda.synchronize() if torch.cuda.is_available() else None
            
            start_time = time.perf_counter()
            for _ in range(num_iterations):
                with torch.no_grad():
                    _ = model(input_ids, attention_mask)
            
            torch.cuda.synchronize() if torch.cuda.is_available() else None
            total_time = time.perf_counter() - start_time
            
            avg_time_ms = (total_time / num_iterations) * 1000
            
            print(f"âœ… ë””ë°”ì´ìŠ¤: {device}")
            print(f"âœ… í‰ê·  ì¶”ë¡  ì‹œê°„: {avg_time_ms:.3f} ms")
            print(f"âœ… ì²˜ë¦¬ëŸ‰: {1000/avg_time_ms:.1f} sequences/sec")
            
            # ëª©í‘œ: 0.1ms ë¯¸ë§Œ (GPU ê¸°ì¤€)
            if device.type == "cuda":
                if avg_time_ms < 0.1:
                    print(f"âœ… ëª©í‘œ ë‹¬ì„±: {avg_time_ms:.3f} ms < 0.1 ms")
                else:
                    print(f"âš ï¸ ëª©í‘œ ë¯¸ë‹¬ì„±: {avg_time_ms:.3f} ms >= 0.1 ms")
            
            self.results["inference_time_ms"] = avg_time_ms
            return True
            
        except Exception as e:
            print(f"âŒ ì¶”ë¡  ì†ë„ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            self.results["inference_speed"] = "FAIL"
            return False
    
    def test_memory_usage(self) -> bool:
        """í…ŒìŠ¤íŠ¸ 7: ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í”„ë¡œíŒŒì¼ë§"""
        print("\n[TEST 7] ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í”„ë¡œíŒŒì¼ë§")
        print("-" * 50)
        
        try:
            # ë©”ëª¨ë¦¬ ì¶”ì  ì‹œì‘
            tracemalloc.start()
            
            # ëª¨ë¸ ìƒì„± ì „ ë©”ëª¨ë¦¬
            snapshot1 = tracemalloc.take_snapshot()
            
            # ëª¨ë¸ ìƒì„±
            model = self.setup_model()
            
            # ëª¨ë¸ ìƒì„± í›„ ë©”ëª¨ë¦¬
            snapshot2 = tracemalloc.take_snapshot()
            
            # ì¶”ë¡  ì‹¤í–‰
            input_ids = torch.randint(1, self.vocab_size, (self.batch_size, self.seq_len))
            attention_mask = torch.ones(self.batch_size, self.seq_len, dtype=torch.long)
            
            with torch.no_grad():
                _ = model(input_ids, attention_mask)
            
            # ì¶”ë¡  í›„ ë©”ëª¨ë¦¬
            snapshot3 = tracemalloc.take_snapshot()
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê³„ì‚°
            model_memory = sum(stat.size_diff for stat in snapshot2.compare_to(snapshot1, 'lineno'))
            inference_memory = sum(stat.size_diff for stat in snapshot3.compare_to(snapshot2, 'lineno'))
            
            model_memory_mb = model_memory / 1024 / 1024
            inference_memory_mb = inference_memory / 1024 / 1024
            total_memory_mb = model_memory_mb + inference_memory_mb
            
            print(f"âœ… ëª¨ë¸ ë©”ëª¨ë¦¬: {model_memory_mb:.2f} MB")
            print(f"âœ… ì¶”ë¡  ë©”ëª¨ë¦¬: {inference_memory_mb:.2f} MB")
            print(f"âœ… ì´ ë©”ëª¨ë¦¬: {total_memory_mb:.2f} MB")
            
            # ëª©í‘œ: 128MB ë¯¸ë§Œ
            assert total_memory_mb < 128, f"ë©”ëª¨ë¦¬ ì´ˆê³¼: {total_memory_mb} MB"
            
            tracemalloc.stop()
            
            self.results["memory_usage_mb"] = total_memory_mb
            return True
            
        except Exception as e:
            print(f"âŒ ë©”ëª¨ë¦¬ í”„ë¡œíŒŒì¼ë§ ì‹¤íŒ¨: {e}")
            self.results["memory_usage"] = "FAIL"
            tracemalloc.stop()
            return False
    
    def test_gradient_flow(self) -> bool:
        """í…ŒìŠ¤íŠ¸ 8: ê·¸ë˜ë””ì–¸íŠ¸ íë¦„ ê²€ì¦"""
        print("\n[TEST 8] ê·¸ë˜ë””ì–¸íŠ¸ íë¦„ í…ŒìŠ¤íŠ¸")
        print("-" * 50)
        
        try:
            model = self.setup_model()
            model.train()
            
            # ë”ë¯¸ ì…ë ¥ ë° íƒ€ê²Ÿ
            input_ids = torch.randint(1, self.vocab_size, (4, 16))
            attention_mask = torch.ones(4, 16, dtype=torch.long)
            targets = torch.randint(0, self.num_classes, (4,))
            
            # ìˆœì „íŒŒ
            outputs = model(input_ids, attention_mask)
            
            # ì†ì‹¤ ê³„ì‚°
            criterion = nn.CrossEntropyLoss()
            loss = criterion(outputs, targets)
            
            # ì—­ì „íŒŒ
            loss.backward()
            
            # ê·¸ë˜ë””ì–¸íŠ¸ í™•ì¸
            gradients_exist = False
            zero_gradients = []
            
            for name, param in model.named_parameters():
                if param.grad is not None:
                    gradients_exist = True
                    if torch.all(param.grad == 0):
                        zero_gradients.append(name)
            
            assert gradients_exist, "ê·¸ë˜ë””ì–¸íŠ¸ê°€ ì „í˜€ ê³„ì‚°ë˜ì§€ ì•ŠìŒ"
            
            if zero_gradients:
                print(f"âš ï¸ 0 ê·¸ë˜ë””ì–¸íŠ¸ íŒŒë¼ë¯¸í„°: {zero_gradients}")
            else:
                print("âœ… ëª¨ë“  íŒŒë¼ë¯¸í„°ì— ìœ íš¨í•œ ê·¸ë˜ë””ì–¸íŠ¸")
            
            print(f"âœ… ì†ì‹¤ê°’: {loss.item():.4f}")
            print("âœ… ê·¸ë˜ë””ì–¸íŠ¸ íë¦„ ì •ìƒ")
            
            self.results["gradient_flow"] = "PASS"
            return True
            
        except Exception as e:
            print(f"âŒ ê·¸ë˜ë””ì–¸íŠ¸ íë¦„ ì‹¤íŒ¨: {e}")
            self.results["gradient_flow"] = "FAIL"
            return False
    
    def test_batch_processing(self) -> bool:
        """í…ŒìŠ¤íŠ¸ 9: ë‹¤ì–‘í•œ ë°°ì¹˜ í¬ê¸° ì²˜ë¦¬"""
        print("\n[TEST 9] ë°°ì¹˜ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸")
        print("-" * 50)
        
        try:
            model = self.setup_model()
            model.eval()
            
            batch_sizes = [1, 8, 32, 64, 128]
            
            for bs in batch_sizes:
                input_ids = torch.randint(1, self.vocab_size, (bs, 32))
                attention_mask = torch.ones(bs, 32, dtype=torch.long)
                
                with torch.no_grad():
                    output = model(input_ids, attention_mask)
                
                assert output.shape == (bs, self.num_classes)
                print(f"  âœ… ë°°ì¹˜ í¬ê¸° {bs:3d}: ì„±ê³µ")
            
            self.results["batch_processing"] = "PASS"
            return True
            
        except Exception as e:
            print(f"âŒ ë°°ì¹˜ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            self.results["batch_processing"] = "FAIL"
            return False
    
    def test_onnx_compatibility(self) -> bool:
        """í…ŒìŠ¤íŠ¸ 10: ONNX ë³€í™˜ ê°€ëŠ¥ì„± í…ŒìŠ¤íŠ¸"""
        print("\n[TEST 10] ONNX í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸")
        print("-" * 50)
        
        try:
            model = self.setup_model()
            model.eval()
            
            # ë”ë¯¸ ì…ë ¥
            dummy_input = torch.randint(1, self.vocab_size, (1, 32))
            dummy_mask = torch.ones(1, 32, dtype=torch.long)
            
            # ONNX ì¶”ì  ì‹œë„
            try:
                torch.onnx.export(
                    model,
                    (dummy_input, dummy_mask),
                    "temp_student.onnx",
                    input_names=['input_ids', 'attention_mask'],
                    output_names=['logits'],
                    dynamic_axes={
                        'input_ids': {0: 'batch_size', 1: 'sequence'},
                        'attention_mask': {0: 'batch_size', 1: 'sequence'},
                        'logits': {0: 'batch_size'}
                    },
                    opset_version=11,
                    verbose=False
                )
                
                # ONNX íŒŒì¼ ì‚­ì œ
                import os
                if os.path.exists("temp_student.onnx"):
                    os.remove("temp_student.onnx")
                
                print("âœ… ONNX ë³€í™˜ ê°€ëŠ¥")
                self.results["onnx_compatible"] = "YES"
                return True
                
            except Exception as onnx_error:
                print(f"âš ï¸ ONNX ë³€í™˜ ì œí•œ: {onnx_error}")
                self.results["onnx_compatible"] = "NO"
                return True  # ONNX ë³€í™˜ì€ ì„ íƒì‚¬í•­ì´ë¯€ë¡œ ì‹¤íŒ¨í•´ë„ í…ŒìŠ¤íŠ¸ëŠ” í†µê³¼
            
        except Exception as e:
            print(f"âŒ ONNX í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            self.results["onnx_compatibility"] = "FAIL"
            return False
    
    def run_all_tests(self):
        """ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ë° ê²°ê³¼ ìš”ì•½"""
        print("=" * 60)
        print("BiLSTM í•™ìƒ ëª¨ë¸ ì¢…í•© í…ŒìŠ¤íŠ¸ ì‹œì‘")
        print("=" * 60)
        
        tests = [
            ("ëª¨ë¸ ìƒì„±", self.test_model_creation),
            ("íŒŒë¼ë¯¸í„° ìˆ˜", self.test_parameter_count),
            ("ì •ìƒ ìˆœì „íŒŒ", self.test_normal_forward),
            ("ê°€ë³€ ê¸¸ì´", self.test_variable_length_sequences),
            ("ì—£ì§€ ì¼€ì´ìŠ¤", self.test_edge_cases),
            ("ì¶”ë¡  ì†ë„", self.test_inference_speed),
            ("ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰", self.test_memory_usage),
            ("ê·¸ë˜ë””ì–¸íŠ¸ íë¦„", self.test_gradient_flow),
            ("ë°°ì¹˜ ì²˜ë¦¬", self.test_batch_processing),
            ("ONNX í˜¸í™˜ì„±", self.test_onnx_compatibility),
        ]
        
        passed = 0
        failed = 0
        
        for test_name, test_func in tests:
            try:
                if test_func():
                    passed += 1
                else:
                    failed += 1
            except Exception as e:
                print(f"í…ŒìŠ¤íŠ¸ {test_name} ì˜ˆì™¸ ë°œìƒ: {e}")
                failed += 1
        
        # ìµœì¢… ê²°ê³¼ ìš”ì•½
        print("\n" + "=" * 60)
        print("í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
        print("=" * 60)
        
        print(f"\nì´ í…ŒìŠ¤íŠ¸: {len(tests)}ê°œ")
        print(f"âœ… í†µê³¼: {passed}ê°œ")
        print(f"âŒ ì‹¤íŒ¨: {failed}ê°œ")
        print(f"ì„±ê³µë¥ : {(passed/len(tests))*100:.1f}%")
        
        # ì£¼ìš” ë©”íŠ¸ë¦­ ì¶œë ¥
        print("\n" + "-" * 60)
        print("ì£¼ìš” ì„±ëŠ¥ ì§€í‘œ")
        print("-" * 60)
        
        if "parameter_count" in self.results and isinstance(self.results["parameter_count"], int):
            print(f"íŒŒë¼ë¯¸í„° ìˆ˜: {self.results['parameter_count']:,}")
        
        if "model_size_mb" in self.results:
            print(f"ëª¨ë¸ í¬ê¸°: {self.results['model_size_mb']:.2f} MB")
        
        if "inference_time_ms" in self.results:
            print(f"ì¶”ë¡  ì‹œê°„: {self.results['inference_time_ms']:.3f} ms")
        
        if "memory_usage_mb" in self.results:
            print(f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {self.results['memory_usage_mb']:.2f} MB")
        
        if "onnx_compatible" in self.results:
            print(f"ONNX í˜¸í™˜: {self.results['onnx_compatible']}")
        
        # ëª©í‘œ ë‹¬ì„± ì—¬ë¶€
        print("\n" + "-" * 60)
        print("í”„ë¡œì íŠ¸ ëª©í‘œ ë‹¬ì„± ì—¬ë¶€")
        print("-" * 60)
        
        goals_met = []
        goals_not_met = []
        
        # íŒŒë¼ë¯¸í„° ìˆ˜ ëª©í‘œ (< 500,000)
        if "parameter_count" in self.results and isinstance(self.results["parameter_count"], int):
            if self.results["parameter_count"] < 500_000:
                goals_met.append("íŒŒë¼ë¯¸í„° ìˆ˜ < 500,000")
            else:
                goals_not_met.append(f"íŒŒë¼ë¯¸í„° ìˆ˜: {self.results['parameter_count']:,} >= 500,000")
        
        # ëª¨ë¸ í¬ê¸° ëª©í‘œ (< 16MB)
        if "model_size_mb" in self.results:
            if self.results["model_size_mb"] < 16:
                goals_met.append("ëª¨ë¸ í¬ê¸° < 16MB")
            else:
                goals_not_met.append(f"ëª¨ë¸ í¬ê¸°: {self.results['model_size_mb']:.2f} MB >= 16MB")
        
        # ì¶”ë¡  ì‹œê°„ ëª©í‘œ (< 0.1ms on GPU)
        if "inference_time_ms" in self.results:
            if torch.cuda.is_available() and self.results["inference_time_ms"] < 0.1:
                goals_met.append("ì¶”ë¡  ì‹œê°„ < 0.1ms (GPU)")
            elif not torch.cuda.is_available():
                goals_met.append(f"ì¶”ë¡  ì‹œê°„: {self.results['inference_time_ms']:.3f} ms (CPU)")
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª©í‘œ (< 128MB)
        if "memory_usage_mb" in self.results:
            if self.results["memory_usage_mb"] < 128:
                goals_met.append("ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ < 128MB")
            else:
                goals_not_met.append(f"ë©”ëª¨ë¦¬: {self.results['memory_usage_mb']:.2f} MB >= 128MB")
        
        print("\në‹¬ì„±í•œ ëª©í‘œ:")
        for goal in goals_met:
            print(f"  âœ… {goal}")
        
        if goals_not_met:
            print("\në¯¸ë‹¬ì„± ëª©í‘œ:")
            for goal in goals_not_met:
                print(f"  âŒ {goal}")
        
        # ìµœì¢… í‰ê°€
        print("\n" + "=" * 60)
        if failed == 0:
            print("ğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼! í”„ë¡œë•ì…˜ ì¤€ë¹„ ì™„ë£Œ!")
        elif failed <= 2:
            print("âš ï¸ ì¼ë¶€ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨. ê²€í†  í•„ìš”.")
        else:
            print("âŒ ë‹¤ìˆ˜ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨. ìˆ˜ì • í•„ìš”.")
        print("=" * 60)


if __name__ == "__main__":
    import traceback
    
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    tester = TestStudentModel()
    tester.run_all_tests()